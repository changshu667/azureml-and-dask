{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a DASK cluster with RAPIDS\n",
    "\n",
    "This notebook runs a DASK cluster with NVIDIA RAPIDS. RAPIDS uses NVIDIA CUDA for high-performance GPU execution, exposing GPU parallelism and high memory bandwidth through a user-friendly Python interface. It includes a dataframe library called cuDF which will be familiar to Pandas users, as well as an ML library called cuML that provides GPU versions of all machine learning algorithms available in Scikit-learn. \n",
    "\n",
    "This notebook shows how through DASK, RAPIDS can take advantage of multi-node, multi-GPU configurations on AzureML. \n",
    "\n",
    "This notebook is deploying the AzureML cluster to a VNet. Prior to running this, setup a VNet and DSVM according to [../setup-vnet.md](../setup-vnet.md). In this case the following names are used to identify the VNet and subnet.\n",
    "\n",
    "In addition, you need to forward the following ports to the DSVM \n",
    "\n",
    "- port 8888 to port 8888 for the jupyter server running on the DSVM (see [../setup-vnet.md](../setup-vnet.md))\n",
    "- port 9999 to port 9999 for the jupyter server running on the AML Cluster (will be explained below)\n",
    "- port 9797 to port 9797 for the jupyter server running on the AML Cluster (will be explained below)\n",
    "\n",
    "The easiert way to accomplish that is by logging into the DSVM using ssh with the following flags (assuming `mydsvm.westeurope.cloudapp.azure.com` is the DNS name for your DSVM:\n",
    "\n",
    "    ssh mydsvm.westeurope.cloudapp.azure.com -L 9797:localhost:9797 -L 9999:localhost:9999 -L 8888:localhost:8888\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import RunConfiguration, MpiConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "class PortForwarder():\n",
    "    '''A helper to forward ports from the Notebook VM to the AML Cluster in the same VNet'''\n",
    "    active_instances = set()\n",
    "    \n",
    "    def __init__(self, from_port, to_ip, to_port):\n",
    "        self.from_port = from_port\n",
    "        self.to_ip = to_ip\n",
    "        self.to_port = to_port\n",
    "        \n",
    "    def start(self):\n",
    "        self._socat = Popen([\"/usr/bin/socat\", \n",
    "                             f\"tcp-listen:{self.from_port},reuseaddr,fork\", \n",
    "                             f\"tcp:{self.to_ip}:{self.to_port}\"],\n",
    "                            stderr=PIPE, stdout=PIPE, universal_newlines=True)\n",
    "        PortForwarder.active_instances.add(self)\n",
    "        return self\n",
    "        \n",
    "    def stop(self):\n",
    "        PortForwarder.active_instances.remove(self)\n",
    "        return self._socat.terminate()\n",
    "    \n",
    "    def stop_all():\n",
    "        for instance in list(PortForwarder.active_instances):\n",
    "            instance.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_cluster_name = \"nd12-vnet-clustr\"\n",
    "vnet_resourcegroup_name='demo'\n",
    "vnet_name='myvnet'\n",
    "subnet_name='default'\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the AmlCompute cluster\n",
    "The next cell is deploying the AmlCompute cluster. The cluster is configured to scale down to 0 nodes after 2 minuten, so no cost is incurred while DASK is not running (and thus no nodes are spun up on the cluster as the result of this cell, yet). This cell only needs to be executed once and the cluster can be reused going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cluster\")\n",
    "\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_ND12s\", \n",
    "        min_nodes=0, \n",
    "        max_nodes=10,\n",
    "        idle_seconds_before_scaledown=120,\n",
    "        vnet_resourcegroup_name=vnet_resourcegroup_name,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name=subnet_name\n",
    "    )\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "\n",
    "    print(\"waiting for nodes\")\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data to Azure Blob Storage\n",
    "\n",
    "This next cell is pulling the NYC taxi data set down and then uploads it to the AzureML workspace's default data store. The all nodes of the DASK cluster we are creating further down will then be able to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-02.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-03.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-04.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-05.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-06.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-07.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-08.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-09.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-10.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-11.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-12.csv\n",
      "- File already exists locally\n",
      "- Uploading taxi data... \n",
      "Uploading an estimated of 12 files\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-09.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-10.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-07.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-12.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-06.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-03.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-01.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-02.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-08.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-05.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-04.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-11.csv\n",
      "Uploaded 0 files\n",
      "- Data transfer complete\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(cwd, 'data'))\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "taxidir = os.path.join(data_dir, 'nyctaxi')\n",
    "if not os.path.exists(taxidir):\n",
    "    os.makedirs(taxidir)\n",
    "\n",
    "filenames = []\n",
    "local_paths = []\n",
    "for i in range(1, 13):\n",
    "    filename = \"yellow_tripdata_2015-{month:02d}.csv\".format(month=i)\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    local_path = os.path.join(taxidir, filename)\n",
    "    local_paths.append(local_path)\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    url = \"http://dask-data.s3.amazonaws.com/nyc-taxi/2015/\" + filename\n",
    "    print(\"- Downloading \" + url)\n",
    "    if not os.path.exists(local_paths[idx]):\n",
    "        with open(local_paths[idx], 'wb') as file:\n",
    "            with urllib.request.urlopen(url) as resp:\n",
    "                length = int(resp.getheader('content-length'))\n",
    "                blocksize = max(4096, length // 100)\n",
    "                with tqdm(total=length, file=sys.stdout) as pbar:\n",
    "                    while True:\n",
    "                        buff = resp.read(blocksize)\n",
    "                        if not buff:\n",
    "                            break\n",
    "                        file.write(buff)\n",
    "                        pbar.update(len(buff))\n",
    "    else:\n",
    "        print(\"- File already exists locally\")\n",
    "\n",
    "print(\"- Uploading taxi data... \")\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(\n",
    "    src_dir=taxidir,\n",
    "    target_path='nyctaxi',\n",
    "    show_progress=True)\n",
    "\n",
    "print(\"- Data transfer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DASK Cluster\n",
    "\n",
    "On the AMLCompute cluster we are now running a Python job that will run a DASK cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "mpi_config = MpiConfiguration()\n",
    "mpi_config.process_count_per_node = 2\n",
    "\n",
    "est = Estimator(\n",
    "    source_directory='./dask',\n",
    "    compute_target=gpu_cluster,\n",
    "    entry_script='init-dask.py',\n",
    "    script_params={\n",
    "        '--data': ws.get_default_datastore(),\n",
    "        '--gpus': str(2)  # The number of GPUs available on each node\n",
    "        },\n",
    "    node_count=3,\n",
    "    use_gpu=True,\n",
    "    distributed_training=mpi_config,\n",
    "    conda_dependencies_file='rapids-0.9.yaml')\n",
    "\n",
    "run = Experiment(ws, \"init-dask-jupyter\").submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the widget to monitor how the DASK cluster spins up. When run for the first time on a workspace, the following thing will happen:\n",
    "\n",
    "1. The docker image will to be created, which takes about 20 minutes. \n",
    "2. Then AzureML will start to scale the cluster up by provisioning the required number of nodes (`node_count` above), which will take another 5-10 minutes with the chosen Standard_ND12s\n",
    "3. The docker image is being transferred over to the compute nodes, which, given the size of about 8 GB takes another 3-5 minutes\n",
    "\n",
    "So alltogether the process will take up to 30 minutes when run for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1f363f16374a4992a6719c9d58b49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the cluster to come up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "it = 0\n",
    "while not \"headnode\" in run.get_metrics():\n",
    "    clear_output(wait=True)\n",
    "    print(\"waiting for scheduler node's ip \" + str(it) )\n",
    "    time.sleep(1)\n",
    "    it += 1\n",
    "\n",
    "headnode = run.get_metrics()[\"headnode\"]\n",
    "jupyter_ip = run.get_metrics()[\"jupyter-ip\"]\n",
    "jupyter_port = run.get_metrics()[\"jupyter-port\"]\n",
    "jupyter_token = run.get_metrics()[\"jupyter-token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish port forwarding to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are forwarding the ports from your local machine as described at the top of this notebook,\n",
      "then you should now be able to connect to the Dashboard and Jupyter Server via the following URLs:\n",
      "\n",
      "     Dashboard: http://localhost:9797\n",
      "     Jupyter on cluster: http://localhost:9999/notebooks/azure_taxi_on_cluster.ipynb?token=0ed225c7db00699b10d80d86dc09d8149d6eae21e7200aac\n"
     ]
    }
   ],
   "source": [
    "dashboard = PortForwarder(9797, headnode, 8787).start()\n",
    "jupyter = PortForwarder(9999, headnode, 8888).start()\n",
    "\n",
    "print(\"If you are forwarding the ports from your local machine as described at the top of this notebook,\")\n",
    "print(\"then you should now be able to connect to the Dashboard and Jupyter Server via the following URLs:\")\n",
    "print()\n",
    "print(f\"     Dashboard: http://localhost:9797\")\n",
    "print(f\"     Jupyter on cluster: http://localhost:9999/notebooks/azure_taxi_on_cluster.ipynb?token={jupyter_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting the cluster down\n",
    "\n",
    "Terminate the run to shut the cluster down. Once you are done with your interactive work, make sure to do this so the AML Compute cluster gets spun down again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the run representing the cluster\n",
    "run.cancel()\n",
    "# shut down the port forwards\n",
    "PortForwarder.stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last run\n",
    "run = Experiment(ws, \"init-dask-jupyter\").get_runs().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headnode': '172.17.0.6',\n",
       " 'scheduler': '172.17.0.6:8786',\n",
       " 'dashboard': '172.17.0.6:8787',\n",
       " 'data': '/mnt/batch/tasks/shared/LS_root/jobs/vnettest/azureml/init-dask-jupyter_1570114867_699d20d4/mounts/workspaceblobstore',\n",
       " 'jupyter-url': 'http://172.17.0.6:8888/?token=0f85e874d045185e175027bab126bd404ebe444c237a765a',\n",
       " 'jupyter-port': 8888,\n",
       " 'jupyter-token': '0f85e874d045185e175027bab126bd404ebe444c237a765a',\n",
       " 'jupyter-ip': '172.17.0.6'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
